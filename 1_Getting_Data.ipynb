{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Getting data\n",
    "------------\n",
    "\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from urllib.request import urlretrieve, urlopen\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "First, download the dataset. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "file_name1 = 'notMNIST_large.tar.gz'\n",
    "file_name2 = 'notMNIST_small.tar.gz'\n",
    "\n",
    "# retreiving size of each files\n",
    "file1_size = int(urlopen(url+file_name1).info().get_all('Content-Length')[0])\n",
    "file2_size = int(urlopen(url+file_name2).info().get_all('Content-Length')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# report hook\n",
    "last_percent_reported = None\n",
    "def reporthook(count, blockSize, totalSize):\n",
    "    \"\"\"\n",
    "        A hook to report the progress of a download. \n",
    "        Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "    \n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"\\r%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def download(url,file_name,size):\n",
    "    #if not os.path.exists('./data'):\n",
    "    #    os.mkdir('./data')\n",
    "    #dest_file = './data/'+file_name\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print('The file',file_name,'is laready downloaded')\n",
    "    else:    \n",
    "        filename, _ = urlretrieve(url+file_name,file_name,reporthook=reporthook)\n",
    "        statinfo = os.stat(file_name)\n",
    "        if statinfo.st_size == size:\n",
    "            print('\\nFound and verified', file_name)\n",
    "        else:\n",
    "            raise Exception(\n",
    "              'Failed to verify ' + file_name + '. Can you get to it with a browser?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%\n",
      "Found and verified notMNIST_large.tar.gz\n",
      "100%\n",
      "Found and verified notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#train_filename, _ = download(url,)\n",
    "\n",
    "download(url,file_name1,file1_size)\n",
    "download(url,file_name2,file2_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Extract the dataset from the compressed .tar.gz file.\n",
    "This should give you a set of directories, labelled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'notMNIST_large'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name1.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large is already present - Skipping extraction.\n",
      "notMNIST_small is already present - Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "def extract(filename):\n",
    "    root = filename.split('.')[0]  # remove .tar.gz\n",
    "    if os.path.exists(root):\n",
    "        print('%s is already present - Skipping extraction.' % root)\n",
    "        return [os.path.join(root,d) for d in sorted(os.listdir(root))\n",
    "                if os.path.isdir(os.path.join(root,d))]\n",
    "    else:\n",
    "        print('Extracting data for %s. Please wait.' % root)\n",
    "        with tarfile.open(filename) as tar:      \n",
    "            tar.extractall('./')\n",
    "        data_folders = [os.path.join(root,d) for d in sorted(os.listdir(root))\n",
    "                if os.path.isdir(os.path.join(root,d))]\n",
    "        if len(data_folders) != num_classes:\n",
    "            raise Exception('Expected %d folders. Found %d instead.' % (\n",
    "                num_classes, len(data_folders)))\n",
    "        print('Extrcation done !')\n",
    "        return data_folders\n",
    "\n",
    "train_folders = extract(file_name1)\n",
    "test_folders  = extract(file_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "Let's display a sample of the images that we just downloaded using IPhyton.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPyCK8gOi/gMFxF4LRt63UQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a16efa4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showimage(label,path):\n",
    "    path = os.path.join(path,label)\n",
    "    files = os.listdir(path)\n",
    "    k = np.random.randint(0,len(files))\n",
    "    X = ndimage.imread(os.path.join(path,files[k]))\n",
    "    plt.imshow(X,cmap=plt.cm.binary)\n",
    "    plt.axis('off');\n",
    "    return X\n",
    "\n",
    "X = showimage('B','notMNIST_small/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Now let's convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. \n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling notMNIST_large/A.pickle.\n",
      "notMNIST_large/A\n",
      "Could not read: notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : cannot identify image file 'notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png' - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : cannot identify image file 'notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png' - it's ok, skipping.\n",
      "Could not read: notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : cannot identify image file 'notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png' - it's ok, skipping.\n",
      "Pickling notMNIST_large/B.pickle.\n",
      "notMNIST_large/B\n",
      "Could not read: notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png : cannot identify image file 'notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png' - it's ok, skipping.\n",
      "Pickling notMNIST_large/C.pickle.\n",
      "notMNIST_large/C\n",
      "Pickling notMNIST_large/D.pickle.\n",
      "notMNIST_large/D\n",
      "Could not read: notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png : cannot identify image file 'notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png' - it's ok, skipping.\n",
      "Pickling notMNIST_large/E.pickle.\n",
      "notMNIST_large/E\n",
      "Pickling notMNIST_large/F.pickle.\n",
      "notMNIST_large/F\n",
      "Pickling notMNIST_large/G.pickle.\n",
      "notMNIST_large/G\n",
      "Pickling notMNIST_large/H.pickle.\n",
      "notMNIST_large/H\n",
      "Pickling notMNIST_large/I.pickle.\n",
      "notMNIST_large/I\n",
      "Pickling notMNIST_large/J.pickle.\n",
      "notMNIST_large/J\n",
      "Pickling notMNIST_small/A.pickle.\n",
      "notMNIST_small/A\n",
      "Could not read: notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png : cannot identify image file 'notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png' - it's ok, skipping.\n",
      "Pickling notMNIST_small/B.pickle.\n",
      "notMNIST_small/B\n",
      "Pickling notMNIST_small/C.pickle.\n",
      "notMNIST_small/C\n",
      "Pickling notMNIST_small/D.pickle.\n",
      "notMNIST_small/D\n",
      "Pickling notMNIST_small/E.pickle.\n",
      "notMNIST_small/E\n",
      "Pickling notMNIST_small/F.pickle.\n",
      "notMNIST_small/F\n",
      "Could not read: notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png : cannot identify image file 'notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png' - it's ok, skipping.\n",
      "Pickling notMNIST_small/G.pickle.\n",
      "notMNIST_small/G\n",
      "Pickling notMNIST_small/H.pickle.\n",
      "notMNIST_small/H\n",
      "Pickling notMNIST_small/I.pickle.\n",
      "notMNIST_small/I\n",
      "Pickling notMNIST_small/J.pickle.\n",
      "notMNIST_small/J\n"
     ]
    }
   ],
   "source": [
    "image_size  = 28         # Pixel width and height.\n",
    "pixel_depth = 255        # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"\n",
    "    Load data for a single letter label.\n",
    "    \"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                        pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images += 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "\n",
    "    dataset = dataset[:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "    return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename):\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "\n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets  = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "vUdbskYE2d87"
   },
   "source": [
    "---\n",
    "Let's verify that the data still looks good by displaying a sample of the labels and images from the ndarray. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_small/J\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a308a5c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%capture\n",
    "letter = load_letter('notMNIST_small/J',1000)\n",
    "plt.imshow(letter[20,:],cmap=plt.cm.binary)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "cYznx5jUwzoO"
   },
   "source": [
    "---\n",
    "Another check: we expect the data to be balanced across classes. Let's verify that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "letters = []\n",
    "for folder in test_folders:\n",
    "    letter = load_letter(folder,1000)\n",
    "    letters.append(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1872, 28, 28)\n",
      "(1873, 28, 28)\n",
      "(1873, 28, 28)\n",
      "(1873, 28, 28)\n",
      "(1873, 28, 28)\n",
      "(1872, 28, 28)\n",
      "(1872, 28, 28)\n",
      "(1872, 28, 28)\n",
      "(1872, 28, 28)\n",
      "(1872, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "for letter in letters:\n",
    "    print(letter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "LA7M7K22ynCt"
   },
   "source": [
    "---\n",
    "Merging and pruning the training data. The labels will be stored into a separate array of integers 0 through 9.\n",
    "We will also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['notMNIST_large/A.pickle',\n",
       " 'notMNIST_large/B.pickle',\n",
       " 'notMNIST_large/C.pickle',\n",
       " 'notMNIST_large/D.pickle',\n",
       " 'notMNIST_large/E.pickle',\n",
       " 'notMNIST_large/F.pickle',\n",
       " 'notMNIST_large/G.pickle',\n",
       " 'notMNIST_large/H.pickle',\n",
       " 'notMNIST_large/I.pickle',\n",
       " 'notMNIST_large/J.pickle']"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels  = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    \n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "    \n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    \n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "\n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "                print('Unable to process data from', pickle_file, ':', e)\n",
    "                raise\n",
    "\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "          train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "tIQJaJuwg5Hw"
   },
   "source": [
    "Finally, let's save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "QiR_rETzem6C"
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels' : train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels' : valid_labels,\n",
    "        'test_dataset' : test_dataset,\n",
    "        'test_labels'  : test_labels\n",
    "    }\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 659 M\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', round(statinfo.st_size/1024/1024),'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('notMNIST.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    for key, val in data.items():\n",
    "        exec(key+'=val')\n",
    "    del(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "---\n",
    "Let's get an idea of what an off-the-shelf classifier can give us on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
    "\n",
    "We will train a simple model on this data using 50, 100, 1000 and 5000 training samples using LogisticRegression model from sklearn.linear_model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "indx = np.random.permutation(5000)\n",
    "N = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.7719\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "model1 = LR() \n",
    "model1.fit(train_dataset[indx,:].reshape([-1,N]),train_labels[indx])\n",
    "\n",
    "y_pred = model1.predict(valid_dataset.reshape([-1,N]))\n",
    "print('Logistic regression accuracy:',accuracy_score(valid_labels,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer Perceptron accuracy: 0.8183\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer Perceptron classifier\n",
    "model2 =MLPClassifier()\n",
    "model2.fit(train_dataset[indx,:].reshape([-1,N]),train_labels[indx])\n",
    "\n",
    "y_pred = model2.predict(valid_dataset.reshape([-1,N]))\n",
    "print('Multi-layer Perceptron accuracy:',accuracy_score(valid_labels,y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
